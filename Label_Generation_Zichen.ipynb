{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Label Generation Zichen.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zn0qDZmz4CXy",
        "outputId": "107cb6e3-082c-4a25-c676-2bb2c7527651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: simple_colors in /usr/local/lib/python3.7/dist-packages (0.1.5)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from google.colab import drive\n",
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "import scipy\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "ETF=pd.read_csv('/content/drive/Shareddrives/Capstone B1/Spinnaker Data/US Sector Inst ETF.csv')\n",
        "Institutional=pd.read_csv('/content/drive/Shareddrives/Capstone B1/Spinnaker Data/US Sector Institutional MF.csv')\n",
        "Retail=pd.read_csv('/content/drive/Shareddrives/Capstone B1/Spinnaker Data/US Sector Retail MF.csv')\n",
        "sp500=pd.read_csv('/content/drive/Shareddrives/Capstone B1/Spinnaker Data/sp500index.csv')"
      ],
      "metadata": {
        "id": "BLQhUaIhhkc8"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get rolling average of a specific column for differect industries\n",
        "# table is the data table you need to calculate the rolling average\n",
        "# window_length means how many rows you want to include in one average calculation\n",
        "# on what is the variable you what to calculate the rolling average\n",
        "# new_column_name is the name of the new column of the rolling average\n",
        "def get_rolling_average(table,window_length,on_what,new_column_name='rollingaverage'):\n",
        "  result=pd.DataFrame(columns=table.columns)\n",
        "  for i in table['AssetClass'].unique():\n",
        "    Retailrolling=table[table['AssetClass']==i]\n",
        "    Retailrolling[new_column_name]=Retailrolling.rolling(window=window_length, center=False, on=on_what, axis=0, closed=None)[on_what].mean()\n",
        "    result=pd.concat([result,Retailrolling])\n",
        "  return result\n",
        "\n",
        "# generate the label varibale based on the threshold\n",
        "# it would return 1 for those values higher than upper bound, -1 for those lower than lower threshold, and 0 for others\n",
        "def get_label(table_orginal,upper=0.05,lower=-0.05):\n",
        "  def get_label_inner_function(change):\n",
        "    if change>upper:\n",
        "      return 1\n",
        "    elif change<lower:\n",
        "      return -1\n",
        "    elif np.isnan(change):\n",
        "      return np.nan\n",
        "    else:\n",
        "      return 0\n",
        "  \n",
        "  table=table_orginal\n",
        "  table['change']=table['rollingaverage']/table['AssetsEnd']-1\n",
        "  table['label']=table['change'].apply(get_label_inner_function)\n",
        "  table_orginal['label']=table['label']\n",
        "  return table_orginal\n",
        "\n",
        "# clean the report date data to an integer\n",
        "def clean_reportdate(s):\n",
        "  s=s.replace(' 12:00:00 AM', '')\n",
        "  s_list=s.split('/')\n",
        "  result=str(s_list[2])+str(s_list[0]).rjust(2,'0')+str(s_list[1]).rjust(2,'0')\n",
        "  return int(result)\n",
        "\n",
        "# clean the sp500 data to the same format so that we could merge it with our data\n",
        "def clean_date_sp599(s):\n",
        "  import calendar\n",
        "  s_list=s.split('-')\n",
        "  if int(s_list[2])<=18:\n",
        "    s_list[2]='20'+s_list[2]\n",
        "  else:\n",
        "    s_list[2]='19'+s_list[2]\n",
        "  return int(s_list[2]+str(list(calendar.month_abbr).index(s_list[1])).rjust(2,'0')+s_list[0])\n",
        "\n",
        "# calculate the weekly change of sp500 index\n",
        "def get_sp_change(table):\n",
        "  result=pd.DataFrame(columns=table.columns)\n",
        "  for i in table['AssetClass'].unique():\n",
        "    spchange=table[table['AssetClass']==i]\n",
        "    spchange['sp_change']=(spchange['close']/(spchange['close'].shift(periods=-1,axis=0))-1)*100\n",
        "    result=pd.concat([result,spchange])\n",
        "  return result\n",
        "\n",
        "# calculate the excess return based on the idea of CAPM and using regression for each industry\n",
        "def get_excess_change(table):\n",
        "  result=pd.DataFrame(columns=table.columns)\n",
        "  lm_result=dict()\n",
        "  for i in table['AssetClass'].unique():\n",
        "    subtable=table[table['AssetClass']==i].dropna(axis=0)\n",
        "    x=np.array(subtable['sp_change'])\n",
        "    X = sm.add_constant(x)\n",
        "    y=np.array(subtable['PortfolioChangePct'])\n",
        "    lm = sm.OLS(y, X).fit()\n",
        "    lm_result[i]=lm\n",
        "    LinearRegression()\n",
        "    subtable['excess_change']=subtable[['PortfolioChangePct']]-LinearRegression().fit(subtable[['sp_change']],subtable[['PortfolioChangePct']]).predict(subtable[['PortfolioChangePct']])\n",
        "    result=pd.concat([result,subtable])\n",
        "  return result,lm_result\n",
        "\n",
        "def merge_sp(table):\n",
        "  table['ReportDate']=table['ReportDate'].apply(clean_reportdate)\n",
        "  sp500=pd.read_csv('/content/drive/Shareddrives/Capstone B1/Spinnaker Data/sp500index.csv')\n",
        "  sp500['date']=sp500['date'].apply(clean_date_sp599)\n",
        "  table=table.merge(sp500,left_on='ReportDate',right_on='date',how='left')\n",
        "  return table\n",
        "\n",
        "def get_t_test_label(table):\n",
        "  table=merge_sp(table)\n",
        "  table=get_sp_change(table)\n",
        "  table,lm_result=get_excess_change(table)\n",
        "  table['t_test']=table.rolling(window=25, center=False, on='excess_change', axis=0, closed=None)['excess_change'].apply(t_test)\n",
        "  return table\n",
        "\n",
        "def t_test(data):\n",
        "  test_result=scipy.stats.ttest_1samp(data, popmean=0, nan_policy='omit')\n",
        "  return test_result[1]"
      ],
      "metadata": {
        "id": "DkIjfZO148Gn"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Retail_assetend_rolling=get_rolling_average(Retail,25,'AssetsEnd')\n",
        "Retail_assetend_rolling=get_label(Retail_assetend_rolling)\n",
        "Retail_assetend_rolling.to_csv('Retail_assetend_rolling.csv')\n",
        "\n",
        "Institutional_assetend_rolling=get_rolling_average(Institutional,25,'AssetsEnd')\n",
        "Institutional_assetend_rolling=get_label(Institutional_assetend_rolling)\n",
        "Institutional_assetend_rolling.to_csv('Institutional_assetend_rolling.csv')\n",
        "\n",
        "ETF_assetend_rolling=get_rolling_average(ETF,25,'AssetsEnd')\n",
        "ETF_assetend_rolling=get_label(ETF_assetend_rolling)\n",
        "ETF_assetend_rolling.to_csv('ETF_assetend_rolling.csv')"
      ],
      "metadata": {
        "id": "4KShoF2u7zs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Retail_excess=get_t_test_label(Retail)\n",
        "Retail_excess.to_csv('Retail_excess_t_test.csv')\n",
        "\n",
        "Institutional_excess=get_t_test_label(Institutional)\n",
        "Institutional_excess.to_csv('Institutional_excess_t_test.csv')\n",
        "\n",
        "ETF_excess=get_t_test_label(ETF)\n",
        "ETF_excess.to_csv('ETF_excess_t_test.csv')"
      ],
      "metadata": {
        "id": "wPjecXi-gS8y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}